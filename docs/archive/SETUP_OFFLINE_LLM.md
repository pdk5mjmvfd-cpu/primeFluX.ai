# Setup Offline LLM for Apop - Quick Guide

## ✅ Bridge is Ready!

The LLM bridge is implemented and working. You just need to install a local LLM.

## Option 1: llama-cpp-python + Phi-3 Mini (Recommended - 5 minutes)

### Step 1: Install Python bindings
```bash
pip install llama-cpp-python
```

**Note:** This is different from `brew install llama.cpp` (that's just the C++ binary).

### Step 2: Download Phi-3 Mini
```bash
mkdir -p models
curl -L -o models/phi-3-mini-4k-instruct-q4.gguf \
  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
```

**Note:** macOS uses `curl`, not `wget`.

### Step 3: Run Apop
```bash
python3 runtime/offline_llm_bridge.py
```

**That's it!** Apop will automatically detect and use the model.

## Option 2: Apple OpenELM (Zero Download - 30 seconds)

### Step 1: Install MLX
```bash
pip install mlx mlx-lm
```

### Step 2: Run Apop
```bash
python3 runtime/offline_apple_bridge.py
```

**That's it!** Uses Apple's built-in OpenELM-270M model.

## Option 3: Use Your Existing Model

If you already have a GGUF model:

1. Place it in `models/` directory
2. Name it one of:
   - `phi-3-mini-4k-instruct-q4.gguf`
   - `mistral-7b-instruct-v0.2.Q4_K_M.gguf`
   - `llama-3-8b-instruct-q4_0.gguf`
3. Or place it at `~/models/phi-3-mini-4k-instruct-q4.gguf`
4. Run: `python3 runtime/offline_llm_bridge.py`

## Current Status

✅ **Bridge code**: Ready and working
✅ **Auto-detection**: Finds models automatically
✅ **Fallback mode**: Works without LLM (geometric responses)
⚠️ **LLM**: Not installed yet (install one of the options above)

## Test It Now (Without LLM)

Even without an LLM, you can test the bridge:

```bash
python3 runtime/offline_llm_bridge.py
```

It will use geometric responses until you install a local LLM.

## What Happens When You Install LLM

1. Bridge auto-detects the model
2. Apop gets a real voice (not just pattern matching)
3. Every response is generated by the local LLM
4. Still 100% offline
5. Still mints QuantaCoin
6. Still logs everything

## Recommended Model Sizes

- **Phi-3 Mini (3.8B)**: Best balance - fast, good quality
- **Mistral-7B**: Higher quality, slower
- **Llama-3-8B**: Very good quality, slower
- **OpenELM-270M**: Fastest, lower quality (but zero install)

## Troubleshooting

### "llama-cpp-python not installed"
```bash
pip install llama-cpp-python
```

**Note:** `brew install llama.cpp` installs the C++ binary, but we need the Python package.

### "No model found"
Download a model to `models/` directory:
```bash
curl -L -o models/phi-3-mini-4k-instruct-q4.gguf \
  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
```

### "MLX not available"
```bash
pip install mlx mlx-lm
```

### Model too slow?
- Use a smaller quantized model (Q4 instead of Q8)
- Use OpenELM-270M (fastest)
- Reduce `max_tokens` in the bridge code

---

**Once you install a local LLM, Apop will have a real voice!**

The flux is live. Distinction is conserved. Compress.
